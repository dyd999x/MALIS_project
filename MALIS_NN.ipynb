{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    path_train_b = \"data/train/benign\"\n",
    "    path_train_m =\"data/train/malignant\"\n",
    "    path_test_b = \"data/test/benign\"\n",
    "    path_test_m = \"data/test/malignant\"\n",
    "    paths = [path_train_b, path_train_m , path_test_b, path_test_m]\n",
    "    \n",
    "    x_train_b = []\n",
    "    x_train_m = []\n",
    "    x_test_b = []\n",
    "    x_test_m = []\n",
    "    x = [x_train_b, x_train_m, x_test_b, x_test_m]\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        for filename in os.listdir(paths[i]):\n",
    "            img = cv2.imread(os.path.join(paths[i],filename))\n",
    "            if img is not None:\n",
    "                x[i].append(img)\n",
    "    \n",
    "    [x_train_b,x_train_m,x_test_b,x_test_m] = x\n",
    "    return x_train_b,x_train_m,x_test_b,x_test_m\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_b,x_train_m,x_test_b,x_test_m = load_images()\n",
    "x_train_b = np.array(x_train_b)\n",
    "x_train_m = np.array(x_train_m)\n",
    "x_test_b = np.array(x_test_b)\n",
    "x_test_m = np.array(x_test_m)\n",
    "\n",
    "x_train_b = x_train_b\n",
    "x_train_m = x_train_m\n",
    "x_test_b = x_test_b[0:50]\n",
    "x_test_m = x_test_m[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "y_train_b = np.zeros((x_train_b.shape[0],1), dtype = int)\n",
    "y_train_m = np.ones((x_train_m.shape[0],1), dtype= int)\n",
    "y_test_b = np.zeros((x_test_b.shape[0],1), dtype = int)\n",
    "y_test_m = np.ones((x_test_m.shape[0],1), dtype = int)\n",
    "\n",
    "#merging\n",
    "x_train = np.concatenate((x_train_b,x_train_m), axis = 0)\n",
    "y_train = np.concatenate((y_train_b,y_train_m), axis = 0)\n",
    "x_test = np.concatenate((x_test_b,x_test_m), axis = 0)\n",
    "y_test = np.concatenate((y_test_b,y_test_m), axis = 0)\n",
    "\n",
    "X = np.concatenate((x_train,x_test), axis = 0)\n",
    "Y = np.concatenate((y_train,y_test), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2637\n",
      "Number of testing examples: 100\n",
      "Each image is of size: (224, 224, 3)\n",
      "train_x_orig shape: (2637, 224, 224, 3)\n",
      "train_y shape: (2637, 1)\n",
      "test_x_orig shape: (100, 224, 224, 3)\n",
      "test_y shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "m_train = x_train.shape[0]\n",
    "num_px = x_train.shape[1]\n",
    "m_test = x_test.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(x_train.shape))\n",
    "print (\"train_y shape: \" + str(y_train.shape))\n",
    "print (\"test_x_orig shape: \" + str(x_test.shape))\n",
    "print (\"test_y shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (150528, 2189)\n",
      "test_x's shape: (150528, 548)\n",
      "train_y's shape(1, 2189)\n",
      "test_y's shape(1, 548)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, shuffle = True, random_state = 42)\n",
    "\n",
    "#reshaping\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "#normalize\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "print (\"train_x's shape: \" + str(X_train.shape))\n",
    "print (\"test_x's shape: \" + str(X_test.shape))\n",
    "print( \"train_y's shape\" + str(Y_train.shape))\n",
    "print (\"test_y's shape\" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    \n",
    "    w1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    w2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "   \n",
    "    parameters = {\"W1\": w1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": w2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    a = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    \n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \n",
    "    a = np.maximum(0,z)\n",
    "    \n",
    "    assert(a.shape == z.shape)\n",
    "    \n",
    "    cache = z \n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "   \n",
    "    \n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "   \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "   \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, linear_activation_cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(linear_activation_cache)\n",
    "    AL, linear_activation_cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(linear_activation_cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    cost = np.squeeze(cost)    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "   \n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate, num_iterations):\n",
    "    grads = {}\n",
    "    costs = []                           \n",
    "    m = X.shape[1]                          \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        cost = compute_cost(A2, Y)\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"relu\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"sigmoid\")\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        parameters = update_parameters(parameters, grads, learning_rate);\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 150528\n",
    "n_h = 9\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "rate = 0.007\n",
    "iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(100*np.sum((p == y)/m)))\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(X_train, Y_train, layers_dims, rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.007\n",
      "number of iterations: 500\n",
      "train set:\n",
      "Accuracy: 73.36683417085426\n",
      "test set:\n",
      "Accuracy: 73.54014598540145\n"
     ]
    }
   ],
   "source": [
    "print(\"learning_rate: \" + str(rate))\n",
    "\n",
    "print(\"number of iterations: \" + str(iterations))\n",
    "\n",
    "print(\"train set:\")\n",
    "predictions_train = predict(X_train, Y_train, parameters)\n",
    "print(\"test set:\")\n",
    "prediction_test = predict(X_test, Y_test, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
